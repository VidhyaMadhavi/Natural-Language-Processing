{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://dzone.com/articles/simple-sentiment-analysis-with-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import import_data as id\n",
    "import prepare_data as pre_data\n",
    "import feature_engineering as fe\n",
    "import modelling as mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=================== Sentiment Analysis on Movie Reviews ===================\n",
      "\n",
      "\n",
      "------------------ Data Loading and Preprocessing ------------------\n",
      "Total of 12500 Files is loaded with Positive Sentiment for Training\n",
      "Total of 12500 Files is loaded with Negative Sentiment for Training\n",
      "Total of 12500 Files is loaded with Positive Sentiment for Testing\n",
      "Total of 12500 Files is loaded with Negative Sentiment for Testing\n",
      "\n",
      "\n",
      "---------------------Dataframe of all the training and testing reviews---------------------\n",
      " \n",
      "              file  label                                             review\n",
      "26247  11122_8.txt      1  Fame is one of the best movies I've seen about...\n",
      "35067  7811_10.txt      1  This movie fully deserves to be one of the top...\n",
      "34590  7382_10.txt      1  in a time of predictable movies, in which abou...\n",
      "16668   2501_1.txt      0  I saw this on TV the other nightÂ",
      " or rather I...\n",
      "12196   9728_7.txt      1  I am a huge fan of Simon Pegg and have watched...\n",
      "\n",
      " Data Loading and Preprocessing Completed...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print (\"\\n\\n=================== Sentiment Analysis on Movie Reviews ===================\")\n",
    "    print (\"\\n\\n------------------ Data Loading and Preprocessing ------------------\")\n",
    "    # Data Import/Data Load - Import text files\n",
    "    # Load the Training Data\n",
    "    path = './data_movie_reviews/train/pos/'\n",
    "    train_positiveFiles, train_pos_merge_content = id.import_files(path)\n",
    "    print (\"Total of {} Files is loaded with Positive Sentiment for Training\".format(len(train_positiveFiles)))\n",
    "\n",
    "    path = './data_movie_reviews/train/neg/'\n",
    "    train_negativeFiles, train_neg_merge_content = id.import_files(path)\n",
    "    print(\"Total of {} Files is loaded with Negative Sentiment for Training\".format(len(train_negativeFiles)))\n",
    "\n",
    "    # Load the Testing Data\n",
    "    path = './data_movie_reviews/test/pos/'\n",
    "    test_positiveFiles, test_pos_merge_content = id.import_files(path)\n",
    "    print(\"Total of {} Files is loaded with Positive Sentiment for Testing\".format(len(test_positiveFiles)))\n",
    "\n",
    "    path = './data_movie_reviews/test/neg/'\n",
    "    test_negativeFiles, test_neg_merge_content = id.import_files(path)\n",
    "    print(\"Total of {} Files is loaded with Negative Sentiment for Testing\".format(len(test_negativeFiles)))\n",
    "\n",
    "    print (\"\\n\\n---------------------Dataframe of all the training and testing reviews---------------------\\n \")\n",
    "    reviews = id.create_dataframe(train_pos_merge_content,train_neg_merge_content,\n",
    "                            test_pos_merge_content,test_neg_merge_content,\n",
    "                            train_positiveFiles,train_negativeFiles,\n",
    "                            test_positiveFiles,test_negativeFiles)\n",
    "    \n",
    "    print(\"\\n Data Loading and Preprocessing Completed...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              file  label                                             review\n",
      "26247  11122_8.txt      1  Fame is one of the best movies I've seen about...\n",
      "35067  7811_10.txt      1  This movie fully deserves to be one of the top...\n",
      "34590  7382_10.txt      1  in a time of predictable movies, in which abou...\n",
      "16668   2501_1.txt      0  I saw this on TV the other nightÂ",
      " or rather I...\n",
      "12196   9728_7.txt      1  I am a huge fan of Simon Pegg and have watched...\n"
     ]
    }
   ],
   "source": [
    "print (reviews.head())\n",
    "number_of_rows = 1000\n",
    "reviews = reviews.iloc[ 0: number_of_rows, : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- “ “.join() restores the gap between the words in a line and is applied to each split word once the punctuation marks have been stripped off it by the function translate(str.maketrans('', '', string.punctuation))\n",
    " \n",
    "- All words of length < 4 as well as the stopwords are removed from the above processed text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "message_text = ' '.join(list(reviews[\"review\"].values))\n",
    "message_text = message_text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "all_words=message_text.split()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in all_words if not w in stop_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "design\n",
      "109833\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [w for w in filtered_sentence if len(w)>=4]\n",
    "print (filtered_sentence[10])\n",
    "print (len(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_labels = nltk.pos_tag(filtered_sentence)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(words_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of nouns that are removed 48\n",
      "                    0    1\n",
      "179              khan  NNP\n",
      "196              khan  NNP\n",
      "5113           khanna  NNP\n",
      "7346          kitchen  NNP\n",
      "8611             knew  NNP\n",
      "9785             knox  NNP\n",
      "14769        kabinett  NNP\n",
      "14954           amber  NNP\n",
      "16714            kick  NNP\n",
      "20892            know  NNP\n",
      "26340      misfortune  NNP\n",
      "27584         mockney  NNP\n",
      "29609          zephyr  NNP\n",
      "32544           â¨big  NNP\n",
      "34824           keith  NNP\n",
      "41188            yarn  NNP\n",
      "43455            âin  NNP\n",
      "44068         mamasan  NNP\n",
      "44152          kattan  NNP\n",
      "51998           zenon  NNP\n",
      "56296            kris  NNP\n",
      "56297   kristofferson  NNP\n",
      "56371   kristofferson  NNP\n",
      "58347           maddy  NNP\n",
      "59552          killer  NNP\n",
      "61126           zucco  NNP\n",
      "62315           orton  NNP\n",
      "63704           queue  NNP\n",
      "73166          zombie  NNP\n",
      "76397          valley  NNP\n",
      "79206            zita  NNP\n",
      "83346          molina  NNP\n",
      "83904       gabrielle  NNP\n",
      "86986          killer  NNP\n",
      "87801            kono  NNP\n",
      "90519          walton  NNP\n",
      "92008          farber  NNP\n",
      "95627            kris  NNP\n",
      "96988   kristofferson  NNP\n",
      "97235       zatã´ichi  NNP\n",
      "99214           julie  NNP\n",
      "99442           sober  NNP\n",
      "103833         molina  NNP\n",
      "103891         molina  NNP\n",
      "104956           bush  NNP\n",
      "105904          moron  NNP\n",
      "105914           kyra  NNP\n",
      "107703           zane  NNP\n"
     ]
    }
   ],
   "source": [
    "nouns_to_be_removed = df.loc[df[1]=='NNP']\n",
    "print (\"No of nouns that are removed\", len(nouns_to_be_removed))\n",
    "print (nouns_to_be_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from filtered_sentence remove the nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   179,    196,   5113,   7346,   8611,   9785,  14769,  14954,\n",
       "             16714,  20892,  26340,  27584,  29609,  32544,  34824,  41188,\n",
       "             43455,  44068,  44152,  51998,  56296,  56297,  56371,  58347,\n",
       "             59552,  61126,  62315,  63704,  73166,  76397,  79206,  83346,\n",
       "             83904,  86986,  87801,  90519,  92008,  95627,  96988,  97235,\n",
       "             99214,  99442, 103833, 103891, 104956, 105904, 105914, 107703],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the indexes to be removed\n",
    "nouns_to_be_removed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(filtered_sentence).drop(nouns_to_be_removed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fame', 'best', 'movies', ..., 'heartfilled', 'review',\n",
       "       'mattercheers'], dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.FreqDist(df.loc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## frequency counts\n",
    "# all_words = nltk.FreqDist(filtered_sentence)  # frequency distribution\n",
    "all_words = nltk.FreqDist(df.loc[:,0].values)\n",
    "word_features = list(all_words.keys())[:4000]  # take 3000 words as featutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_of_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>[Fame, best, movies, I've, seen, Performing, A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35067</th>\n",
       "      <td>[This, movie, fully, deserves, Hindi, comedies...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           list_of_words  label\n",
       "26247  [Fame, best, movies, I've, seen, Performing, A...      1\n",
       "35067  [This, movie, fully, deserves, Hindi, comedies...      1"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_reviews_to_words(rev):\n",
    "    list_of_docs_in_words = []\n",
    "    for review in rev:\n",
    "        words = review.split()\n",
    "        filtered = [w for w in words if not w in stop_words and len(w) >=4]\n",
    "        list_of_docs_in_words.append(filtered)\n",
    "    return list_of_docs_in_words\n",
    "    \n",
    "res = split_reviews_to_words(list(reviews['review'].values))\n",
    "reviews['list_of_words'] = res\n",
    "documents = reviews.loc[:, ['list_of_words', 'label']]\n",
    "documents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each word in the document assign true/false.\n",
    "def find_features(document):\n",
    "    words = set(document)   # get list of unique words\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "#(words :(true if in  3000 features)  and category)\n",
    "featuresets = [(find_features(rev), category) for rev, category in documents.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_range = int (number_of_rows/1.3)\n",
    "\n",
    "training_set = featuresets[:training_range]\n",
    "testing_set = featuresets[training_range:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 77.48917748917748\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                0 : 1      =     11.4 : 1.0\n",
      "                   worst = True                0 : 1      =     11.1 : 1.0\n",
      "                    lame = True                0 : 1      =      9.8 : 1.0\n",
      "                straight = True                0 : 1      =      9.8 : 1.0\n",
      "                  wasted = True                0 : 1      =      8.4 : 1.0\n",
      "                   spent = True                0 : 1      =      7.8 : 1.0\n",
      "                 deserve = True                0 : 1      =      7.1 : 1.0\n",
      "                  poorly = True                0 : 1      =      7.1 : 1.0\n",
      "               redeeming = True                0 : 1      =      7.1 : 1.0\n",
      "                   sweet = True                1 : 0      =      6.9 : 1.0\n",
      "                hospital = True                1 : 0      =      6.9 : 1.0\n",
      "                    sent = True                1 : 0      =      6.9 : 1.0\n",
      "                  nudity = True                0 : 1      =      6.4 : 1.0\n",
      "                 treated = True                0 : 1      =      6.4 : 1.0\n",
      "                    dull = True                0 : 1      =      6.4 : 1.0\n",
      "                  rating = True                0 : 1      =      6.3 : 1.0\n",
      "                 fantasy = True                1 : 0      =      6.3 : 1.0\n",
      "                numerous = True                0 : 1      =      5.7 : 1.0\n",
      "                  crappy = True                0 : 1      =      5.7 : 1.0\n",
      "             pretentious = True                0 : 1      =      5.7 : 1.0\n",
      "               basically = True                0 : 1      =      5.4 : 1.0\n",
      "                   fails = True                0 : 1      =      5.4 : 1.0\n",
      "                   loved = True                1 : 0      =      5.2 : 1.0\n",
      "             predictable = True                0 : 1      =      5.1 : 1.0\n",
      "                 seconds = True                0 : 1      =      5.1 : 1.0\n",
      "                  makers = True                0 : 1      =      5.1 : 1.0\n",
      "                    mess = True                0 : 1      =      5.1 : 1.0\n",
      "               offensive = True                0 : 1      =      5.1 : 1.0\n",
      "                  island = True                0 : 1      =      5.1 : 1.0\n",
      "                   offer = True                0 : 1      =      5.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 77.9220779221\n",
      "BernoulliNB_classifier accuracy percent: 78.7878787879\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "\n",
    "####  this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.  here we have positive/negative as boolean features\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algorithm accuracy percent: 77.48917748917748\n",
      "Most Informative Features\n",
      "                   waste = True                0 : 1      =     11.4 : 1.0\n",
      "                   worst = True                0 : 1      =     11.1 : 1.0\n",
      "                    lame = True                0 : 1      =      9.8 : 1.0\n",
      "                straight = True                0 : 1      =      9.8 : 1.0\n",
      "                  wasted = True                0 : 1      =      8.4 : 1.0\n",
      "                   spent = True                0 : 1      =      7.8 : 1.0\n",
      "                 deserve = True                0 : 1      =      7.1 : 1.0\n",
      "                  poorly = True                0 : 1      =      7.1 : 1.0\n",
      "               redeeming = True                0 : 1      =      7.1 : 1.0\n",
      "                   sweet = True                1 : 0      =      6.9 : 1.0\n",
      "                hospital = True                1 : 0      =      6.9 : 1.0\n",
      "                    sent = True                1 : 0      =      6.9 : 1.0\n",
      "                  nudity = True                0 : 1      =      6.4 : 1.0\n",
      "                 treated = True                0 : 1      =      6.4 : 1.0\n",
      "                    dull = True                0 : 1      =      6.4 : 1.0\n",
      "                  rating = True                0 : 1      =      6.3 : 1.0\n",
      "                 fantasy = True                1 : 0      =      6.3 : 1.0\n",
      "                numerous = True                0 : 1      =      5.7 : 1.0\n",
      "                  crappy = True                0 : 1      =      5.7 : 1.0\n",
      "             pretentious = True                0 : 1      =      5.7 : 1.0\n",
      "               basically = True                0 : 1      =      5.4 : 1.0\n",
      "                   fails = True                0 : 1      =      5.4 : 1.0\n",
      "                   loved = True                1 : 0      =      5.2 : 1.0\n",
      "             predictable = True                0 : 1      =      5.1 : 1.0\n",
      "                 seconds = True                0 : 1      =      5.1 : 1.0\n",
      "                  makers = True                0 : 1      =      5.1 : 1.0\n",
      "                    mess = True                0 : 1      =      5.1 : 1.0\n",
      "               offensive = True                0 : 1      =      5.1 : 1.0\n",
      "                  island = True                0 : 1      =      5.1 : 1.0\n",
      "                   offer = True                0 : 1      =      5.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Naive Bayes Algorithm accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 77.0562770563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy percent: 50.2164502165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 71.8614718615\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
