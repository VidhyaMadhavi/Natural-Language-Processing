{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to define a vocabulary of known words when using a bag-of-words model.\n",
    "\n",
    "The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. This is difficult to know beforehand and often it is important to test different hypotheses about how to construct a useful vocabulary.\n",
    "\n",
    "We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words.\n",
    "\n",
    "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document can be added to the counter (a new function called add_doc_to_vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process_docs())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "No_of_Files_to_process = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"latin1\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    count=0\n",
    "    for filename in listdir(directory):\n",
    "        count = count + 1\n",
    "        if count <= No_of_Files_to_process :\n",
    "            # create the full path of the file to open\n",
    "            path = directory + '/' + filename\n",
    "            # load and clean the doc\n",
    "            line = doc_to_line(path, vocab)\n",
    "            # add to list\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "\tscores = list()\n",
    "\tn_repeats = 1\n",
    "\tn_words = Xtest.shape[1]\n",
    "\tfor i in range(n_repeats):\n",
    "\t\t# define network\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t\t# compile network\n",
    "\t\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t\t# fit network\n",
    "\t\tmodel.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
    "\t\t# evaluate\n",
    "\t\tloss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "\t\tscores.append(acc)\n",
    "\t\tprint('%d accuracy: %s' % ((i+1), acc))\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare bag of words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "\t# create the tokenizer\n",
    "\ttokenizer = Tokenizer()\n",
    "\t# fit the tokenizer on the documents\n",
    "\ttokenizer.fit_on_texts(train_docs)\n",
    "\t# encode training data set\n",
    "\tXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "\t# encode training data set\n",
    "\tXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "\treturn Xtrain, Xtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "train_positive_lines = process_docs('data_movie_reviews/train/pos', vocab, True)\n",
    "train_negative_lines = process_docs('data_movie_reviews/train/neg', vocab, True)\n",
    "train_docs = train_positive_lines + train_negative_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "test_positive_lines = process_docs('data_movie_reviews/test/pos', vocab, True)\n",
    "test_negative_lines = process_docs('data_movie_reviews/test/neg', vocab, True)\n",
    "test_docs = test_positive_lines + test_negative_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_positive_lines), len(train_negative_lines), len(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 0 - Negative \n",
    "- ### 1 - Positive\n",
    "- Create labels for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Word Scoring Methods\n",
    "The texts_to_matrix() function for the Tokenizer in the Keras API provides 4 different methods for scoring words; they are:\n",
    "\n",
    "    “binary” Where words are marked as present (1) or absent (0).\n",
    "    “count” Where the occurrence count for each word is marked as an integer.\n",
    "    “tfidf” Where each word is scored based on their frequency, where words that are common across all documents are penalized.\n",
    "    “freq” Where words are scored based on their frequency of occurrence within the document.\n",
    "We can evaluate the skill of the model developed in the previous section fit using each of the 4 supported word scoring modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 1s - loss: 0.6857 - acc: 0.5700\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.4392 - acc: 0.9850\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.2795 - acc: 1.0000\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.1671 - acc: 1.0000\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.1010 - acc: 1.0000\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0630 - acc: 1.0000\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0416 - acc: 1.0000\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0224 - acc: 1.0000\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 48/50\n",
      " - 0s - loss: 9.6556e-04 - acc: 1.0000\n",
      "Epoch 49/50\n",
      " - 0s - loss: 9.2909e-04 - acc: 1.0000\n",
      "Epoch 50/50\n",
      " - 0s - loss: 8.9123e-04 - acc: 1.0000\n",
      "1 accuracy: 0.75\n",
      "       binary\n",
      "count    1.00\n",
      "mean     0.75\n",
      "std       NaN\n",
      "min      0.75\n",
      "25%      0.75\n",
      "50%      0.75\n",
      "75%      0.75\n",
      "max      0.75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ac0f88bdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "# for mode in modes:\n",
    "for mode in ['binary']:\n",
    "    # prepare data for mode\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "    # evaluate model on data for mode\n",
    "    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
