{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to define a vocabulary of known words when using a bag-of-words model.\n",
    "\n",
    "The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. This is difficult to know beforehand and often it is important to test different hypotheses about how to construct a useful vocabulary.\n",
    "\n",
    "We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words.\n",
    "\n",
    "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document can be added to the counter (a new function called add_doc_to_vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process_docs())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138193\n",
      "[('br', 57141), ('The', 44351), ('movie', 41194), ('film', 37006), ('one', 23185), ('like', 18876), ('This', 14725), ('good', 13927), ('It', 12074), ('would', 12010), ('time', 11502), ('really', 11275), ('story', 11055), ('even', 10882), ('see', 10853), ('much', 9289), ('get', 8995), ('people', 8473), ('bad', 8456), ('great', 8260), ('made', 7887), ('first', 7881), ('well', 7844), ('also', 7722), ('make', 7599), ('films', 7595), ('movies', 7586), ('could', 7567), ('way', 7464), ('dont', 7332), ('But', 7216), ('characters', 7183), ('think', 7077), ('Its', 6738), ('And', 6715), ('seen', 6485), ('character', 6454), ('watch', 6269), ('many', 6233), ('two', 6171), ('plot', 6137), ('acting', 6108), ('never', 6101), ('little', 5980), ('know', 5953), ('In', 5880), ('best', 5717), ('love', 5700), ('show', 5683), ('life', 5650)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding=\"latin1\")\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('data_movie_reviews/train/pos', vocab)\n",
    "process_docs('data_movie_reviews/train/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews.\n",
    "\n",
    "For example, the following snippet will retrieve only the tokens that appear 2 or more times in all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65483"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "# min_occurane = 1\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Save the tokens\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding=\"latin1\")\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    count=0\n",
    "    for filename in listdir(directory):\n",
    "        count = count + 1\n",
    "        if count <= 100 :\n",
    "            # create the full path of the file to open\n",
    "            path = directory + '/' + filename\n",
    "            # load and clean the doc\n",
    "            line = doc_to_line(path, vocab)\n",
    "            # add to list\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "\tscores = list()\n",
    "\tn_repeats = 1\n",
    "\tn_words = Xtest.shape[1]\n",
    "\tfor i in range(n_repeats):\n",
    "\t\t# define network\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "\t\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t\t# compile network\n",
    "\t\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t\t# fit network\n",
    "\t\tmodel.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
    "\t\t# evaluate\n",
    "\t\tloss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "\t\tscores.append(acc)\n",
    "\t\tprint('%d accuracy: %s' % ((i+1), acc))\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
      "[[ 0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  3.  2.]\n",
      " [ 0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=3)\n",
    "texts = ['a b b c c c', 'a b c']\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(tokenizer.texts_to_matrix(texts, mode='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  3.,  1.],\n",
       "       [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=4)\n",
    "texts = ['a b b b b c c c', 'a b c']\n",
    "tokenizer.fit_on_texts(texts)\n",
    "matrix = tokenizer.texts_to_matrix(texts, mode='count')\n",
    "matrix[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare bag of words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "\t# create the tokenizer\n",
    "\ttokenizer = Tokenizer()\n",
    "\t# fit the tokenizer on the documents\n",
    "\ttokenizer.fit_on_texts(train_docs)\n",
    "\t# encode training data set\n",
    "\tXtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "\t# encode training data set\n",
    "\tXtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "\treturn Xtrain, Xtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Instead',\n",
       " 'undertakings',\n",
       " 'border',\n",
       " 'Any',\n",
       " 'shadowed',\n",
       " 'finds',\n",
       " 'Profanity',\n",
       " 'doh',\n",
       " 'honesty',\n",
       " 'Britishness',\n",
       " 'Abysmal',\n",
       " 'clods',\n",
       " 'Bendixs',\n",
       " 'senile',\n",
       " 'tushies',\n",
       " 'Unborn',\n",
       " 'onewhich',\n",
       " 'contraceptive',\n",
       " 'Hahahahaha',\n",
       " 'Spoilersbr',\n",
       " 'coveragebr',\n",
       " 'Sloper',\n",
       " 'Retire',\n",
       " 'auteur',\n",
       " 'interaction',\n",
       " 'Markys',\n",
       " 'comb',\n",
       " 'pofaced',\n",
       " 'Longoria',\n",
       " 'paintersbr',\n",
       " 'Cleese',\n",
       " 'summarily',\n",
       " 'reactors',\n",
       " 'IDAP',\n",
       " 'disapproving',\n",
       " 'sparkling',\n",
       " 'Duke',\n",
       " 'TARDIS',\n",
       " 'shootemup',\n",
       " 'Sid',\n",
       " 'snooty',\n",
       " 'Iago',\n",
       " 'Horrific',\n",
       " 'XI',\n",
       " 'Togar',\n",
       " 'hottest',\n",
       " 'runsbr',\n",
       " 'applicable',\n",
       " 'pre',\n",
       " 'Napa',\n",
       " 'Dutton',\n",
       " 'Fellowes',\n",
       " 'ninebr',\n",
       " 'truckload',\n",
       " 'Motivations',\n",
       " 'Declaration',\n",
       " 'Kes',\n",
       " 'combo',\n",
       " 'sculptures',\n",
       " 'Gibbler',\n",
       " 'medallion',\n",
       " 'Choice',\n",
       " 'ceases',\n",
       " 'RELEASE',\n",
       " 'mint',\n",
       " 'arses',\n",
       " 'boor',\n",
       " 'slower',\n",
       " 'GordonLevitt',\n",
       " 'Pedantic',\n",
       " 'scriptwriter',\n",
       " 'Alone',\n",
       " 'MesoAmericans',\n",
       " 'HASNT',\n",
       " 'Disappointingly',\n",
       " 'preach',\n",
       " 'audacity',\n",
       " 'forgivenessbr',\n",
       " 'fitfully',\n",
       " 'Bmovie',\n",
       " 'Cree',\n",
       " 'improvementbr',\n",
       " 'overexposure',\n",
       " 'Teaching',\n",
       " 'avg',\n",
       " 'Curve',\n",
       " 'oddly',\n",
       " 'inhabit',\n",
       " 'rebels',\n",
       " 'riotous',\n",
       " 'swastikas',\n",
       " 'Boxing',\n",
       " 'categories',\n",
       " 'Hoff',\n",
       " 'claws',\n",
       " 'sidekick',\n",
       " 'Kensington',\n",
       " 'goals',\n",
       " 'discrepancy',\n",
       " 'legalize',\n",
       " 'unmitigated',\n",
       " 'onboard',\n",
       " 'specter',\n",
       " 'Morbius',\n",
       " 'Blacksploitation',\n",
       " 'Verdone',\n",
       " 'snacking',\n",
       " 'cavorting',\n",
       " 'DannyPaul',\n",
       " 'batmobile',\n",
       " 'unaccountably',\n",
       " 'Corregidor',\n",
       " 'reeking',\n",
       " 'Gunther',\n",
       " 'Ouedraogos',\n",
       " 'winningly',\n",
       " 'hereon',\n",
       " 'Tootsie',\n",
       " 'Haenel',\n",
       " 'taciturn',\n",
       " 'dainty',\n",
       " 'southerners',\n",
       " 'Majestic',\n",
       " 'timpani',\n",
       " 'Twin',\n",
       " 'Carroll',\n",
       " 'Dors',\n",
       " 'turnbr',\n",
       " 'sadder',\n",
       " 'Reanimator',\n",
       " 'maintained',\n",
       " 'LINCOLN',\n",
       " 'moviemakers',\n",
       " 'masterly',\n",
       " 'unnaturally',\n",
       " 'Jolies',\n",
       " 'Emsworth',\n",
       " 'Expiration',\n",
       " 'munchie',\n",
       " 'ACUs',\n",
       " 'bee',\n",
       " 'Gellars',\n",
       " 'confront',\n",
       " 'Shushui',\n",
       " 'sandstorm',\n",
       " 'STEAL',\n",
       " 'hereIn',\n",
       " 'Mesa',\n",
       " 'Real',\n",
       " 'vengeance',\n",
       " 'Calicos',\n",
       " 'forcible',\n",
       " 'Soho',\n",
       " 'showtime',\n",
       " 'Understanding',\n",
       " 'euthanasia',\n",
       " 'cam',\n",
       " 'Chopping',\n",
       " 'Savalas',\n",
       " 'Girls',\n",
       " 'quizzed',\n",
       " 'shared',\n",
       " 'protofascist',\n",
       " 'backI',\n",
       " 'cleared',\n",
       " 'lounging',\n",
       " 'selfrighteousness',\n",
       " 'Zuniga',\n",
       " 'lemonade',\n",
       " 'operatives',\n",
       " 'turnsbr',\n",
       " 'Once',\n",
       " 'Apes',\n",
       " 'Menon',\n",
       " 'beauty',\n",
       " 'dislocate',\n",
       " 'atticbr',\n",
       " 'pseudoaction',\n",
       " 'vindictively',\n",
       " 'undergrad',\n",
       " 'watchings',\n",
       " 'exclude',\n",
       " 'Verheyen',\n",
       " 'revere',\n",
       " 'segment',\n",
       " 'churches',\n",
       " 'grisly',\n",
       " 'wilds',\n",
       " 'Effort',\n",
       " 'exponentially',\n",
       " 'eminent',\n",
       " 'longest',\n",
       " 'wowed',\n",
       " 'HorrorSciFi',\n",
       " 'Stifler',\n",
       " 'Sadashiv',\n",
       " 'Agustin',\n",
       " 'akward',\n",
       " 'ray',\n",
       " 'Pvt',\n",
       " 'Piero',\n",
       " 'nicelooking',\n",
       " 'Simpsonbr',\n",
       " 'constructive',\n",
       " 'listI',\n",
       " 'Frailty',\n",
       " 'Mulgrew',\n",
       " 'petri',\n",
       " 'butshe',\n",
       " 'Healey',\n",
       " 'situated',\n",
       " 'drift',\n",
       " 'SpiderMan',\n",
       " 'demon',\n",
       " 'tailored',\n",
       " 'Randolphs',\n",
       " 'handmade',\n",
       " 'loftier',\n",
       " 'vary',\n",
       " 'midtwenties',\n",
       " 'illuminations',\n",
       " 'averted',\n",
       " 'Seinfeld',\n",
       " 'arrows',\n",
       " 'decked',\n",
       " 'opposite',\n",
       " 'Celtic',\n",
       " 'allIt',\n",
       " 'Grotesquely',\n",
       " 'flint',\n",
       " 'Passions',\n",
       " 'EXISTENCE',\n",
       " 'versionthe',\n",
       " 'Storms',\n",
       " 'Accorsi',\n",
       " 'FC',\n",
       " 'couplebr',\n",
       " 'Nikolai',\n",
       " 'closed',\n",
       " 'attack',\n",
       " 'SHIELD',\n",
       " 'wonderbr',\n",
       " 'throughbut',\n",
       " 'chimpanzees',\n",
       " 'coherentbr',\n",
       " 'moviehe',\n",
       " 'Flamingo',\n",
       " 'Assassin',\n",
       " 'drybr',\n",
       " 'transvestism',\n",
       " 'Vermin',\n",
       " 'Grated',\n",
       " 'LETS',\n",
       " 'Puberty',\n",
       " 'commissioning',\n",
       " 'fakes',\n",
       " 'frighteningly',\n",
       " 'Trueblood',\n",
       " 'thickly',\n",
       " 'bearable',\n",
       " 'jackhammers',\n",
       " 'fearing',\n",
       " 'invasion',\n",
       " 'darkbr',\n",
       " 'Lisbeth',\n",
       " 'cliquebr',\n",
       " 'Witnesses',\n",
       " 'followup',\n",
       " 'irritatingbr',\n",
       " 'Particular',\n",
       " 'Daria',\n",
       " 'EPISODE',\n",
       " 'lookbr',\n",
       " 'nightclubbr',\n",
       " 'Giancarlo',\n",
       " 'ballistic',\n",
       " 'Expertly',\n",
       " 'Salkow',\n",
       " 'Corrine',\n",
       " 'Tightly',\n",
       " 'hegemony',\n",
       " 'vulnerabilities',\n",
       " 'deathly',\n",
       " 'ted',\n",
       " 'Nukem',\n",
       " 'pseudodocumentary',\n",
       " 'vocational',\n",
       " 'mismarketed',\n",
       " 'Marjories',\n",
       " 'Future',\n",
       " 'waxes',\n",
       " 'Budget',\n",
       " 'daddys',\n",
       " 'shriveled',\n",
       " 'Government',\n",
       " 'harried',\n",
       " 'SNOW',\n",
       " 'Rychard',\n",
       " 'specimen',\n",
       " 'Wanters',\n",
       " 'wages',\n",
       " 'Nang',\n",
       " 'Aliens',\n",
       " 'camerashots',\n",
       " 'tranquilizer',\n",
       " 'Aishu',\n",
       " 'respected',\n",
       " 'Centered',\n",
       " 'today',\n",
       " 'trumpets',\n",
       " 'averaged',\n",
       " 'strands',\n",
       " 'Bigardo',\n",
       " 'Normally',\n",
       " 'advertisementsbr',\n",
       " 'maturity',\n",
       " 'swimmer',\n",
       " 'Niagara',\n",
       " 'Bleibtreu',\n",
       " 'voyage',\n",
       " 'chapterplay',\n",
       " 'unharvested',\n",
       " 'Hallen',\n",
       " 'Bounty',\n",
       " 'Creasey',\n",
       " 'Adventure',\n",
       " 'prolly',\n",
       " 'mess',\n",
       " 'Disneys',\n",
       " 'bloodshedbr',\n",
       " 'Mathurin',\n",
       " 'picturisation',\n",
       " 'foyer',\n",
       " 'WATCHED',\n",
       " 'dominos',\n",
       " 'unwilling',\n",
       " 'pleasure',\n",
       " 'compliant',\n",
       " 'motion',\n",
       " 'Gallery',\n",
       " 'mythologybr',\n",
       " 'punchline',\n",
       " 'meteorites',\n",
       " 'neighborhoodbr',\n",
       " 'rescuers',\n",
       " 'Denzels',\n",
       " 'yiddish',\n",
       " 'harpy',\n",
       " 'abusive',\n",
       " 'Cate',\n",
       " 'documenting',\n",
       " 'charactor',\n",
       " 'magical',\n",
       " 'ye',\n",
       " 'Buffalo',\n",
       " 'proudbr',\n",
       " 'connection',\n",
       " 'watchI',\n",
       " 'Apples',\n",
       " 'cheesiness',\n",
       " 'Sembene',\n",
       " 'Excellence',\n",
       " 'possession',\n",
       " 'linereading',\n",
       " 'Mildreds',\n",
       " 'renditions',\n",
       " 'latters',\n",
       " 'editions',\n",
       " 'Africabr',\n",
       " 'ogre',\n",
       " 'TNN',\n",
       " 'mocks',\n",
       " 'Abkani',\n",
       " 'sexobsessed',\n",
       " 'Madhavis',\n",
       " 'lasagna',\n",
       " 'Snowy',\n",
       " 'Marilla',\n",
       " 'dishwater',\n",
       " 'lurching',\n",
       " 'Schiller',\n",
       " 'Kellaway',\n",
       " 'Merlinbr',\n",
       " 'awestruck',\n",
       " 'Busch',\n",
       " 'stakesbr',\n",
       " 'tempts',\n",
       " 'Elga',\n",
       " 'preschoolers',\n",
       " 'Bloom',\n",
       " 'Rep',\n",
       " 'carry',\n",
       " 'Koslack',\n",
       " 'zany',\n",
       " 'Rodger',\n",
       " 'Collinwood',\n",
       " 'plum',\n",
       " 'Emory',\n",
       " 'Mukherjee',\n",
       " 'collie',\n",
       " 'waned',\n",
       " 'backwoods',\n",
       " 'discover',\n",
       " 'bribe',\n",
       " 'Rataud',\n",
       " 'processing',\n",
       " 'Tweed',\n",
       " 'Jester',\n",
       " 'enlightening',\n",
       " 'Castlebr',\n",
       " 'Neffs',\n",
       " 'criticismsbr',\n",
       " 'skimp',\n",
       " 'obedient',\n",
       " 'ponder',\n",
       " 'decadebr',\n",
       " 'whichever',\n",
       " 'Jodorowsky',\n",
       " 'fates',\n",
       " 'researcher',\n",
       " 'sassy',\n",
       " 'microbes',\n",
       " 'shamebr',\n",
       " 'ravens',\n",
       " 'brandishing',\n",
       " 'Jared',\n",
       " 'choreograph',\n",
       " 'regularlybr',\n",
       " 'Condor',\n",
       " 'perished',\n",
       " 'prostituting',\n",
       " 'tapped',\n",
       " 'Footage',\n",
       " 'assholes',\n",
       " 'Hondas',\n",
       " 'convinces',\n",
       " 'Ouedraogo',\n",
       " 'nudists',\n",
       " 'warden',\n",
       " 'Adriens',\n",
       " 'coalition',\n",
       " 'Leopard',\n",
       " 'Tustin',\n",
       " 'Gretchen',\n",
       " 'improv',\n",
       " 'Shit',\n",
       " 'suitbr',\n",
       " 'retardation',\n",
       " 'blood',\n",
       " 'thrills',\n",
       " 'buildingbr',\n",
       " 'RC',\n",
       " 'chanced',\n",
       " 'groggy',\n",
       " 'Used',\n",
       " 'Democratic',\n",
       " 'NEED',\n",
       " 'WIDE',\n",
       " 'availability',\n",
       " 'compensating',\n",
       " 'layed',\n",
       " 'emanating',\n",
       " 'drop',\n",
       " 'Neighbor',\n",
       " 'SpecialFX',\n",
       " 'Lyndon',\n",
       " 'nonaggression',\n",
       " 'rockin',\n",
       " 'Rehab',\n",
       " 'Halliburton',\n",
       " 'TOY',\n",
       " 'profitmaking',\n",
       " 'nutcracker',\n",
       " 'Mander',\n",
       " 'Leesville',\n",
       " 'Moroccan',\n",
       " 'suitable',\n",
       " 'amorphous',\n",
       " 'Danas',\n",
       " 'Gypsy',\n",
       " 'loosing',\n",
       " 'Bolger',\n",
       " 'Shecky',\n",
       " 'motors',\n",
       " 'Honestlythe',\n",
       " 'perplex',\n",
       " 'Yamadas',\n",
       " 'Sturges',\n",
       " 'Dancers',\n",
       " 'INCREDIBLY',\n",
       " 'yr',\n",
       " 'Mayers',\n",
       " 'griever',\n",
       " 'PI',\n",
       " 'heating',\n",
       " 'insignificant',\n",
       " 'Busbys',\n",
       " 'beige',\n",
       " 'specialbr',\n",
       " 'tumbles',\n",
       " 'dusty',\n",
       " 'eightbr',\n",
       " 'Fragata',\n",
       " 'abysmalbr',\n",
       " 'echt',\n",
       " 'Planes',\n",
       " 'neglecting',\n",
       " 'receipt',\n",
       " 'ungainly',\n",
       " 'atonement',\n",
       " 'streamlined',\n",
       " 'Goines',\n",
       " 'ruin',\n",
       " 'airbag',\n",
       " 'Bicycle',\n",
       " 'borrowings',\n",
       " 'Branagh',\n",
       " 'caveman',\n",
       " 'judgemental',\n",
       " 'Feinstones',\n",
       " 'SPEAK',\n",
       " 'GOODBYE',\n",
       " 'infected',\n",
       " 'snoops',\n",
       " 'focusedbr',\n",
       " 'herbs',\n",
       " 'affectionbr',\n",
       " 'savoured',\n",
       " 'compulsively',\n",
       " 'locationsbr',\n",
       " 'incompetence',\n",
       " 'Simms',\n",
       " 'downpour',\n",
       " 'impasse',\n",
       " 'alarmist',\n",
       " 'sufferingbr',\n",
       " 'copping',\n",
       " 'tightens',\n",
       " 'Jehovahs',\n",
       " 'scenesthe',\n",
       " 'Dez',\n",
       " 'Buried',\n",
       " 'Crosbys',\n",
       " 'outrageousness',\n",
       " 'longlimbed',\n",
       " 'actionviolence',\n",
       " 'Nova',\n",
       " 'proviolence',\n",
       " 'BEL',\n",
       " 'positionbr',\n",
       " 'shunted',\n",
       " 'contract',\n",
       " 'mutantsbr',\n",
       " 'VIII',\n",
       " 'semiclad',\n",
       " 'Recent',\n",
       " 'rescinded',\n",
       " 'decaying',\n",
       " 'squadron',\n",
       " 'Helmut',\n",
       " 'trenchant',\n",
       " 'plod',\n",
       " 'supervise',\n",
       " 'tried',\n",
       " 'Alert',\n",
       " 'tastelessness',\n",
       " 'planebr',\n",
       " 'Spurlock',\n",
       " 'Youre',\n",
       " 'sadism',\n",
       " 'Zulu',\n",
       " 'flaring',\n",
       " 'thisThe',\n",
       " 'grandson',\n",
       " 'longhaired',\n",
       " 'grandfather',\n",
       " 'reveres',\n",
       " 'mogul',\n",
       " 'Global',\n",
       " 'Hasnt',\n",
       " 'Tran',\n",
       " 'tiresomely',\n",
       " 'hardboiled',\n",
       " 'Everyday',\n",
       " 'Rodney',\n",
       " 'perfect',\n",
       " 'Simpson',\n",
       " 'Whitlock',\n",
       " 'Quarter',\n",
       " 'Cammareri',\n",
       " 'Cato',\n",
       " 'thunderstorm',\n",
       " 'Live',\n",
       " 'mourning',\n",
       " 'hereand',\n",
       " 'unlawful',\n",
       " 'ANC',\n",
       " 'acerbic',\n",
       " 'Bradburys',\n",
       " 'Fathers',\n",
       " 'biblethumping',\n",
       " 'Massudes',\n",
       " 'Depending',\n",
       " 'divorced',\n",
       " 'degradation',\n",
       " 'Dodger',\n",
       " 'truism',\n",
       " 'Rough',\n",
       " 'Daddy',\n",
       " 'tingle',\n",
       " 'usable',\n",
       " 'nonebr',\n",
       " 'Rougebr',\n",
       " 'longstanding',\n",
       " 'resuming',\n",
       " 'Gackts',\n",
       " 'detracts',\n",
       " 'Milton',\n",
       " 'asteroid',\n",
       " 'agencies',\n",
       " 'Cosmatos',\n",
       " 'Stay',\n",
       " 'Dunno',\n",
       " 'nurture',\n",
       " 'flowered',\n",
       " 'Adolph',\n",
       " 'Gouden',\n",
       " 'dawning',\n",
       " 'endowments',\n",
       " 'Considered',\n",
       " 'movement',\n",
       " 'Katharine',\n",
       " 'piece',\n",
       " 'overactive',\n",
       " 'Deodato',\n",
       " 'indulgences',\n",
       " 'tangent',\n",
       " 'Discovery',\n",
       " 'bounded',\n",
       " 'fancied',\n",
       " 'sloppybr',\n",
       " 'peril',\n",
       " 'Member',\n",
       " 'Dales',\n",
       " 'Sox',\n",
       " 'Scratch',\n",
       " 'funnyor',\n",
       " 'CinemaScope',\n",
       " 'swearI',\n",
       " 'financially',\n",
       " 'wax',\n",
       " 'Woodward',\n",
       " 'beachbr',\n",
       " 'naturalbr',\n",
       " 'forseeable',\n",
       " 'thermostat',\n",
       " 'Perfectbr',\n",
       " 'CUTS',\n",
       " 'consume',\n",
       " 'Anhalt',\n",
       " 'strained',\n",
       " 'Starstruck',\n",
       " 'Mortenson',\n",
       " 'childhood',\n",
       " 'Guilty',\n",
       " 'pragmatic',\n",
       " 'coarseness',\n",
       " 'fairness',\n",
       " 'watersbr',\n",
       " 'Boar',\n",
       " 'screens',\n",
       " 'transplantation',\n",
       " 'footages',\n",
       " 'Katsopolis',\n",
       " 'armament',\n",
       " 'glares',\n",
       " 'Shootings',\n",
       " 'lamely',\n",
       " 'Postino',\n",
       " 'SHEWOLF',\n",
       " 'fastpaced',\n",
       " 'SalaameIshq',\n",
       " 'Concert',\n",
       " 'Dinocroc',\n",
       " 'cultists',\n",
       " 'invitations',\n",
       " 'Burke',\n",
       " 'Jox',\n",
       " 'render',\n",
       " 'Hennessey',\n",
       " 'naughty',\n",
       " 'headphones',\n",
       " 'appear',\n",
       " 'Want',\n",
       " 'timeit',\n",
       " 'revisionist',\n",
       " 'Marin',\n",
       " 'report',\n",
       " 'Guardian',\n",
       " 'strap',\n",
       " 'Prophecy',\n",
       " 'EMPIRE',\n",
       " 'watchlist',\n",
       " 'Conde',\n",
       " 'specialty',\n",
       " 'notalent',\n",
       " 'rustlers',\n",
       " 'GRANDMA',\n",
       " 'McDonell',\n",
       " 'Degree',\n",
       " 'pacifistic',\n",
       " 'haranguing',\n",
       " 'Laputabr',\n",
       " 'cod',\n",
       " 'careful',\n",
       " 'rapture',\n",
       " 'Milner',\n",
       " 'typesbr',\n",
       " 'Gangrene',\n",
       " 'nonsexual',\n",
       " 'Superdome',\n",
       " 'Rashad',\n",
       " 'createbr',\n",
       " 'missing',\n",
       " 'Hollywood',\n",
       " 'hardy',\n",
       " 'army',\n",
       " 'misguided',\n",
       " 'alarmingbr',\n",
       " 'earn',\n",
       " 'materialistic',\n",
       " 'wittiness',\n",
       " 'ethically',\n",
       " 'Takechis',\n",
       " 'Donavon',\n",
       " 'Ellies',\n",
       " 'restricts',\n",
       " 'supplemental',\n",
       " 'Summary',\n",
       " 'Agi',\n",
       " 'digressions',\n",
       " 'Shelob',\n",
       " 'maninthemac',\n",
       " 'Lifetimebr',\n",
       " 'centipede',\n",
       " 'Ignore',\n",
       " 'WAX',\n",
       " 'flybr',\n",
       " 'Slys',\n",
       " 'belle',\n",
       " 'van',\n",
       " 'laden',\n",
       " 'Gonzales',\n",
       " 'HUSBAND',\n",
       " 'UNCUT',\n",
       " 'Hag',\n",
       " 'warbut',\n",
       " 'tensions',\n",
       " 'places',\n",
       " 'IV',\n",
       " 'hippieera',\n",
       " 'confidante',\n",
       " 'National',\n",
       " 'crystalline',\n",
       " 'greek',\n",
       " 'Meatball',\n",
       " 'Treveiler',\n",
       " 'timespace',\n",
       " 'oneAnd',\n",
       " 'Losey',\n",
       " 'dried',\n",
       " 'rewind',\n",
       " 'stunning',\n",
       " 'ughbr',\n",
       " 'Fozzie',\n",
       " 'humane',\n",
       " 'Teenagers',\n",
       " 'contextualized',\n",
       " 'Myrtle',\n",
       " 'fellowimdb',\n",
       " 'Vaugier',\n",
       " 'Veeru',\n",
       " 'regalia',\n",
       " 'satirized',\n",
       " 'Willem',\n",
       " 'affiliate',\n",
       " 'strife',\n",
       " 'Subway',\n",
       " 'Wilders',\n",
       " 'escapes',\n",
       " 'props',\n",
       " 'Menzies',\n",
       " 'movieIncredibly',\n",
       " 'batting',\n",
       " 'pickup',\n",
       " 'Despite',\n",
       " 'movienight',\n",
       " 'scheming',\n",
       " 'Marischka',\n",
       " 'Hells',\n",
       " 'slapstickbr',\n",
       " 'Lance',\n",
       " 'Zeenat',\n",
       " 'Stopping',\n",
       " 'wig',\n",
       " 'wellread',\n",
       " 'Jhorror',\n",
       " 'endlessly',\n",
       " 'Scenario',\n",
       " 'impersonating',\n",
       " 'bravery',\n",
       " 'Oslo',\n",
       " 'implausible',\n",
       " 'Boca',\n",
       " 'latrine',\n",
       " 'Lilian',\n",
       " 'sensationalist',\n",
       " 'Racing',\n",
       " 'ego',\n",
       " 'Notebook',\n",
       " 'primal',\n",
       " 'heirs',\n",
       " 'Gaines',\n",
       " 'Rosanna',\n",
       " 'groovy',\n",
       " 'Gotham',\n",
       " 'overseeing',\n",
       " 'corruptionbr',\n",
       " 'Romano',\n",
       " 'Congressional',\n",
       " 'Impulse',\n",
       " 'overexaggerated',\n",
       " 'endeavoring',\n",
       " 'manor',\n",
       " 'morale',\n",
       " 'Dooleys',\n",
       " 'Beckett',\n",
       " 'puppybr',\n",
       " 'unsolved',\n",
       " 'Koran',\n",
       " 'Shame',\n",
       " 'unkind',\n",
       " 'kinetic',\n",
       " 'Fools',\n",
       " 'climaxing',\n",
       " 'blurted',\n",
       " 'noticeably',\n",
       " 'manhunt',\n",
       " 'welljudged',\n",
       " 'startle',\n",
       " 'Stranglerbr',\n",
       " 'Hunk',\n",
       " 'magictrain',\n",
       " 'unforgettable',\n",
       " 'plotpoints',\n",
       " 'goround',\n",
       " 'Thoroughly',\n",
       " 'territorial',\n",
       " 'copycats',\n",
       " 'Blindpassasjer',\n",
       " 'Teal',\n",
       " 'Almost',\n",
       " 'Beth',\n",
       " 'Bushmans',\n",
       " 'Matrixstyle',\n",
       " 'issuebr',\n",
       " 'boisterously',\n",
       " 'IIAscension',\n",
       " 'Hatred',\n",
       " 'Explosive',\n",
       " 'intriguing',\n",
       " 'Languagebr',\n",
       " 'lovestruck',\n",
       " 'Swamp',\n",
       " 'Tarintino',\n",
       " 'Alliances',\n",
       " 'Word',\n",
       " 'flapping',\n",
       " 'Viktor',\n",
       " 'Mansion',\n",
       " 'Gespenster',\n",
       " 'looser',\n",
       " 'farcical',\n",
       " 'Maya',\n",
       " 'Cuaron',\n",
       " 'kin',\n",
       " 'Frankensteins',\n",
       " 'reptiles',\n",
       " 'deservebr',\n",
       " 'documentarist',\n",
       " 'Larrazabal',\n",
       " 'Hughes',\n",
       " 'Rings',\n",
       " 'Tessie',\n",
       " 'asked',\n",
       " 'percentages',\n",
       " 'Hurrah',\n",
       " 'steppers',\n",
       " 'Flux',\n",
       " 'capacity',\n",
       " 'Manuccie',\n",
       " 'anything',\n",
       " 'chins',\n",
       " 'jour',\n",
       " 'haystack',\n",
       " 'Palestinians',\n",
       " 'stepdaughters',\n",
       " 'Zierings',\n",
       " 'INCIDENT',\n",
       " 'Princess',\n",
       " 'Toshio',\n",
       " 'statesman',\n",
       " 'reunited',\n",
       " 'prop',\n",
       " 'Gareth',\n",
       " 'Larrys',\n",
       " 'Cmdr',\n",
       " 'Elvirabr',\n",
       " 'DIVA',\n",
       " 'Annabel',\n",
       " 'Englishdubbed',\n",
       " 'designerbr',\n",
       " 'Hemlich',\n",
       " 'Cabo',\n",
       " 'pressuring',\n",
       " 'leftbr',\n",
       " 'Rajni',\n",
       " 'Morphin',\n",
       " 'popularist',\n",
       " 'Halima',\n",
       " 'outoftouch',\n",
       " 'husbandandwife',\n",
       " 'parades',\n",
       " 'valkyries',\n",
       " 'chord',\n",
       " 'Bushman',\n",
       " 'overweight',\n",
       " 'LIPSTICK',\n",
       " 'NZ',\n",
       " 'eastmeetswest',\n",
       " 'Gymkata',\n",
       " 'inflated',\n",
       " 'Clarence',\n",
       " 'masterpiecesbr',\n",
       " 'oddballs',\n",
       " 'drunks',\n",
       " 'Carlitos',\n",
       " 'Spotlight',\n",
       " 'slag',\n",
       " 'Kara',\n",
       " 'Misadventures',\n",
       " 'arent',\n",
       " 'Mickey',\n",
       " 'exposing',\n",
       " 'CHARLES',\n",
       " 'Jess',\n",
       " 'clones',\n",
       " 'playful',\n",
       " 'mines',\n",
       " 'counterbalance',\n",
       " 'CHRISTOPHER',\n",
       " 'Unexpectedly',\n",
       " 'boos',\n",
       " 'characterizations',\n",
       " 'sightseeing',\n",
       " 'ceiling',\n",
       " 'SAVAGE',\n",
       " 'Ardh',\n",
       " 'underneath',\n",
       " 'doddering',\n",
       " 'Cronkite',\n",
       " 'Cong',\n",
       " 'Gantry',\n",
       " 'hoofing',\n",
       " 'brims',\n",
       " 'Lacanians',\n",
       " 'supersaturated',\n",
       " 'bewarebr',\n",
       " 'madebr',\n",
       " 'upped',\n",
       " 'budge',\n",
       " 'Naomi',\n",
       " 'gravitate',\n",
       " 'Crystal',\n",
       " 'Bonnies',\n",
       " 'sunbathing',\n",
       " 'LindumSvendsens',\n",
       " 'outweighs',\n",
       " 'gowns',\n",
       " 'devotee',\n",
       " 'pulsepounding',\n",
       " 'Petersburg',\n",
       " 'Valhalla',\n",
       " 'reaped',\n",
       " 'agonies',\n",
       " 'exhaustive',\n",
       " 'hots',\n",
       " 'trilogybr',\n",
       " 'hearkening',\n",
       " 'bashful',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "train_positive_lines = process_docs('data_movie_reviews/train/pos', vocab, True)\n",
    "train_negative_lines = process_docs('data_movie_reviews/train/neg', vocab, True)\n",
    "train_docs = train_positive_lines + train_negative_lines\n",
    "# import pickle\n",
    "# with open('train_positive_lines.pk', 'wb') as fp:\n",
    "#     pickle.dump( train_positive_lines, fp)\n",
    "# with open('train_positive_lines.pk', 'rb') as fp:\n",
    "#     data = pickle.load( fp)\n",
    "# # print (data[0:2])\n",
    "# import pickle\n",
    "# with open('train_negative_lines.pk', 'wb') as fp:\n",
    "#     pickle.dump( train_negative_lines, fp)\n",
    "# with open('train_negative_lines.pk', 'rb') as fp:\n",
    "#     data = pickle.load( fp)\n",
    "# # print (data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "test_positive_lines = process_docs('data_movie_reviews/test/pos', vocab, True)\n",
    "test_negative_lines = process_docs('data_movie_reviews/test/neg', vocab, True)\n",
    "test_docs = test_positive_lines + test_negative_lines\n",
    "# import pickle\n",
    "# with open('test_positive_lines.pk', 'wb') as fp:\n",
    "#     pickle.dump( test_positive_lines, fp)\n",
    "# with open('test_positive_lines.pk', 'rb') as fp:\n",
    "#     data = pickle.load( fp)\n",
    "# # print (data[0:2])\n",
    "# import pickle\n",
    "# with open('test_negative_lines.pk', 'wb') as fp:\n",
    "#     pickle.dump( test_negative_lines, fp)\n",
    "# with open('test_negative_lines.pk', 'rb') as fp:\n",
    "#     data = pickle.load( fp)\n",
    "# # print (data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_positive_lines), len(train_negative_lines), len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0 for _ in range(10)]\n",
    "[1 for _ in range(10)]\n",
    "[0 for _ in range(10)] + [1 for _ in range(10)]\n",
    "array([0 for _ in range(10)] + [1 for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 0 - Negative \n",
    "- ### 1 - Positive\n",
    "- Create labels for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\ttokenizer = Tokenizer()\n",
    "\t# fit the tokenizer on the documents\n",
    "\ttokenizer.fit_on_texts(train_docs)\n",
    "# \t# encode training data set\n",
    "\tXtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "    \n",
    "# \t# encode training data set\n",
    "# \tXtest = tokenizer.texts_to_matrix(test_docs, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368025000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a bytes object larger than 4 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f33e67f9d2af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Xtrain_binary.pk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Xtrain_binary.pk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot serialize a bytes object larger than 4 GiB"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('Xtrain_binary.pk', 'wb') as fp:\n",
    "    pickle.dump( Xtrain, fp)\n",
    "with open('Xtrain_binary.pk', 'rb') as fp:\n",
    "    data = pickle.load( fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Word Scoring Methods\n",
    "The texts_to_matrix() function for the Tokenizer in the Keras API provides 4 different methods for scoring words; they are:\n",
    "\n",
    "    “binary” Where words are marked as present (1) or absent (0).\n",
    "    “count” Where the occurrence count for each word is marked as an integer.\n",
    "    “tfidf” Where each word is scored based on their frequency, where words that are common across all documents are penalized.\n",
    "    “freq” Where words are scored based on their frequency of occurrence within the document.\n",
    "We can evaluate the skill of the model developed in the previous section fit using each of the 4 supported word scoring modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# # encode training data set\n",
    "tokenizer.texts_to_matrix(train_docs, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   4.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   1., ...,   0.,   0.,   0.],\n",
       "       ..., \n",
       "       [  0.,   0.,   5., ...,   0.,   0.,   0.],\n",
       "       [  0.,  11.,   1., ...,   0.,   0.,   0.],\n",
       "       [  0.,   5.,   1., ...,   1.,   1.,   1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# # encode training data set\n",
    "tokenizer.texts_to_matrix(train_docs, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.01843318,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01098901, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.05555556, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.02689487,  0.00244499, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.0617284 ,  0.01234568, ...,  0.01234568,\n",
       "         0.01234568,  0.01234568]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# # encode training data set\n",
    "tokenizer.texts_to_matrix(train_docs, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  2.28013157,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.90885575, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  2.37160266, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  3.24672782,  0.90885575, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  2.49334779,  0.90885575, ...,  4.61512052,\n",
       "         4.61512052,  4.61512052]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# # encode training data set\n",
    "tokenizer.texts_to_matrix(train_docs, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.01843318,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.01098901, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.05555556, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.02689487,  0.00244499, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.0617284 ,  0.01234568, ...,  0.01234568,\n",
       "         0.01234568,  0.01234568]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# # encode training data set\n",
    "tokenizer.texts_to_matrix(train_docs, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 1s - loss: 0.6751 - acc: 0.5650\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.4128 - acc: 0.9950\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.2552 - acc: 1.0000\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.1507 - acc: 1.0000\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.0903 - acc: 1.0000\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0577 - acc: 1.0000\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0395 - acc: 1.0000\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0212 - acc: 1.0000\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0170 - acc: 1.0000\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 45/50\n",
      " - 0s - loss: 9.7502e-04 - acc: 1.0000\n",
      "Epoch 46/50\n",
      " - 0s - loss: 9.3458e-04 - acc: 1.0000\n",
      "Epoch 47/50\n",
      " - 0s - loss: 9.0015e-04 - acc: 1.0000\n",
      "Epoch 48/50\n",
      " - 0s - loss: 8.6780e-04 - acc: 1.0000\n",
      "Epoch 49/50\n",
      " - 0s - loss: 8.3341e-04 - acc: 1.0000\n",
      "Epoch 50/50\n",
      " - 0s - loss: 8.0270e-04 - acc: 1.0000\n",
      "1 accuracy: 0.73\n",
      "       binary\n",
      "count    1.00\n",
      "mean     0.73\n",
      "std       NaN\n",
      "min      0.73\n",
      "25%      0.73\n",
      "50%      0.73\n",
      "75%      0.73\n",
      "max      0.73\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWvj4WzkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b48adcc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "# for mode in modes:\n",
    "for mode in ['binary']:\n",
    "\t# prepare data for mode\n",
    "\tXtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "\t# evaluate model on data for mode\n",
    "\tresults[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
